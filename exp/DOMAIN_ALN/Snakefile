# CONFIGURATION FILE ---------------------------------------------------------
configfile: 'config.yaml'
# ----------------------------------------------------------------------------

from os.path import basename, dirname


# PROTEIN FAMILY PARAMETERS --------------------------------------------------

FAMILIES            = config.get('gene_families')
HMM_DB              = config.get('hmm_db')
PROTEIN_SEQUENCES   = config.get('protein_sequences')
HMMPRESS_OUT        = ['h3f', 'h3i', 'h3m', 'h3p']

# GENERAL VARIABLES ----------------------------------------------------------

CORES               = int(snakemake.get_argument_parser().parse_args().cores or 1)
OUT_DIR             = basename(FAMILIES).rsplit('.', 1)[0]
SCRIPT_DIR          = config.get('script_dir', 'scripts')

# ----------------------------------------------------------------------------

def get_domain_files():
    import pandas as pd
    df = pd.read_csv(f'{OUT_DIR}/proteins.domains.besthit.nooverlap.tsv', sep='\t', header=[0])

    return df.apply(lambda x: f'{OUT_DIR}/domains/{x.family}.{x.hit_accession}.faa', axis=1).unique().tolist()

def get_aln_files_for_all_domains():
    import pandas as pd
    df = pd.read_csv(f'{OUT_DIR}/proteins.domains.besthit.nooverlap.tsv', sep='\t', header=[0])

    return df.apply(lambda x: f'{OUT_DIR}/domains/{x.family}.{x.hit_accession}.aln', axis=1).unique().tolist()

def get_aln_files_for_all_families():
    import pandas as pd
    df = pd.read_csv(f'{OUT_DIR}/proteins.domains.besthit.nooverlap.tsv', sep='\t', header=[0])

    return expand(f'{OUT_DIR}/{{family}}.phy', family=df.family.unique())

def get_aln_files_for_family(wildcards):
    import pandas as pd
    df = pd.read_csv(f'{OUT_DIR}/proteins.domains.besthit.nooverlap.tsv', sep='\t', header=[0])

    return expand(f'{OUT_DIR}/domains/{wildcards.family}.{{acc}}.aln', acc=df.loc[df.family == wildcards.family].hit_accession.unique())


def get_profile_files(wildcards):
    import pandas as pd
    df = pd.read_csv(f'{OUT_DIR}/proteins.domains.besthit.nooverlap.tsv', sep='\t', header=[0])

    return df.hit_accession.map(lambda x: f'{dirname(HMM_DB)}/profiles/{x}.hmm').unique().tolist()


# INITIAL RULE ---------------------------------------------------------------
rule all:
    input:
        get_aln_files_for_all_families()

rule familes_to_df:
    input:
        FAMILIES
    output:
        f'{OUT_DIR}/families.tsv'
    run:
        from itertools import repeat, chain
        import pandas as pd
        import csv
        df = pd.DataFrame(data=chain(*map(lambda x: zip(x[1].split(), repeat(x[0])), csv.reader(open(input[0]), delimiter='\t'))),
                                columns=['gene_id', 'family'])
        with open(output[0], 'w') as out:
            df.to_csv(out, sep='\t', index=False)


rule hmmpress:
    input:
        HMM_DB
    output:
        expand(f'{HMM_DB}.{{out}}', out=HMMPRESS_OUT)
    conda:
        'workflow-env.yaml'
    log:
        f'{HMM_DB}.hmmpress.log'
    shell:
        'hmmpress {input} > {log}'


rule extract_protein_sequences:
    input:
        families = f'{OUT_DIR}/families.tsv',
        protein_seqs = PROTEIN_SEQUENCES,
    output:
        f'{OUT_DIR}/proteins.faa',
    run:
        import pandas as pd

        df_fam = pd.read_csv(input.families, sep='\t', header=[0], index_col=[0])
        df_prot = pd.read_csv(input.protein_seqs, sep='\t', header=[0])

        df_prot['species_gene_id'] = df_prot.apply(lambda x: x.Organism.replace(' ', '') + '|' + x['Gene ID'], axis=1)
        df_prot.set_index('species_gene_id', inplace=True)
        with open(output[0], 'w') as out:
            for idx, gene in df_prot.loc[df_fam.index].iterrows():
                print(f'>{idx}\n{gene["Predicted Protein Sequence"]}', file=out)


rule hmmscan:
    input:
        hmm_db = HMM_DB,
        _ = expand(f'{HMM_DB}.{{out}}', out=HMMPRESS_OUT),
        protein_seqs = f'{OUT_DIR}/proteins.faa'
    output:
        f'{OUT_DIR}/proteins.domtblout'
    conda:
        'workflow-env.yaml'
    log:
        f'{OUT_DIR}/proteins.domtblout.log'
    threads:
        CORES
    shell:
        'hmmscan --cpu {threads} -o {log} --domtblout {output} {input.hmm_db} {input.protein_seqs}'


rule domtblout_to_besthit_table:
    input:
        domtblout = f'{OUT_DIR}/proteins.domtblout',
        families = f'{OUT_DIR}/families.tsv'
    output:
        f'{OUT_DIR}/proteins.domains.besthit.tsv'
    log:
        f'{OUT_DIR}/proteins.domains.besthit.log'
    run:
        import pandas as pd
        from Bio import SearchIO

        df_fam = pd.read_csv(input.families, sep='\t', header=[0], index_col=[0])

        data = list()
        for res in SearchIO.parse(open(input.domtblout), 'hmmscan3-domtab'):
            for hit in res.hits:
                # select best on conditional evalue:
                # statistical significance of the domain given that we know that the sequence is a true homolog
                hsp = min(hit, key=lambda x: x.evalue_cond)
                data.append((hsp.query_id, hsp.query_start, hsp.query_end, hit.accession, hit.id, hit.description, hsp.hit_start, hsp.hit_end,
                             hsp.evalue_cond, hsp.bitscore))
        df = pd.DataFrame(data = data, columns=['query_id', 'query_start', 'query_end', 'hit_accession', 'hit_id', 'hit_description', 'hit_start',
                                                'hit_end', 'conditional_evalue', 'bitscore'])
        df.set_index('query_id', inplace=True)
        df['family'] = df_fam.loc[df.index].family
        with open(log[0], 'w') as logging:
            print(f'{len(df_fam.index.difference(df.index))} out of {df_fam.shape[0]} genes have no significant hits to PFAM domains, ' + \
                    f'affecting {len(df_fam.loc[df_fam.index.difference(df.index)].family.unique())} out of {len(df_fam.family.unique())} families, ' + \
                    f'out of which {len(set(df_fam.family.unique()).difference(df.family))} have no domain hit in any of its members', file=logging)
        with open(output[0], 'w') as out:
            df.reset_index().to_csv(out, sep='\t', index=False)


rule filter_overlapping_besthits:
    input:
        besthits = f'{OUT_DIR}/proteins.domains.besthit.tsv',
    output:
        f'{OUT_DIR}/proteins.domains.besthit.nooverlap.tsv'
    run:
        from itertools import combinations
        import networkx as nx
        import pandas as pd
        import numpy as np

        df_bh = pd.read_csv(input.besthits, sep='\t', header=[0], index_col=[0])
        df_bh['query_interval'] = pd.IntervalIndex.from_arrays(df_bh['query_start'], df_bh['query_end'], closed='left')
        # there will be a warning here regarding log'ing zeros, but that will be fine, because the outcome is -inf and that ensures that the
        # corresponding elements are minimal and therefore chosen first
        df_bh['conditional_evalue_log'] = df_bh.conditional_evalue.map(np.log)

        df = None

        for fam in df_bh.family.unique():
            # sorting ensures that the best hit will remain in df_acc when grouping by index
            df_f = df_bh.loc[df_bh.family == fam].sort_values('conditional_evalue_log')
            # construct overlap graph
            G = nx.Graph()
            G.add_nodes_from(df_f.hit_accession.unique())
            for a, b in combinations(G.nodes(), 2):
                df_a = df_f.loc[df_f.hit_accession == a]
                df_b = df_f.loc[df_f.hit_accession == b]
                for query in df_a.index.intersection(df_b.index):
                    if df_a.loc[query].query_interval.overlaps(df_b.loc[query].query_interval):
                        G.add_edge(a, b)
                        break

            # we sort values so that the best groups (with of log(evalue) is smallest) are processed first
            df_hits = df_f[['hit_accession', 'conditional_evalue_log']].groupby('hit_accession').sum().sort_values('conditional_evalue_log')

            # covered is a flag that ensures that only the best out of all overlapping domains are chosen
            df_hits['covered']=False
            for acc in df_hits.index:
                if not df_hits.covered[acc]:
                    df_acc = df_f.loc[df_f.hit_accession == acc].groupby(level=0).first().reset_index()
                    if df is None:
                        df = df_acc
                    else:
                        df = pd.concat([df, df_acc])
                    # choosing the complete connected component of the overlap graph may not be the best choice, but so far it's been fine
                    df_hits.loc[list(nx.node_connected_component(G, acc)), 'covered'] = True

        df.drop(['conditional_evalue_log', 'query_interval'], axis=1, inplace=True)
        with open(output[0], 'w') as out:
            df.to_csv(out, sep='\t', index=False)


rule produce_domain_faa_per_family:
    input:
        domains = f'{OUT_DIR}/proteins.domains.besthit.nooverlap.tsv',
        proteins = f'{OUT_DIR}/proteins.faa',
        profiles_dummy = get_profile_files,
    output:
        get_domain_files()
    run:
        from os.path import dirname
        from Bio import SeqIO
        import pandas as pd

        seqs = dict(map(lambda x: (x.id, x), SeqIO.parse(open(input.proteins), format='fasta')))
        df = pd.read_csv(input.domains, sep='\t', header=[0])
        df.set_index(['family', 'hit_accession'], inplace=True)

        for fam, acc in df.index.unique():
            with open(f'{dirname(output[0])}/{fam}.{acc}.faa', 'w') as out:
                for _, row in df.loc[(fam, acc)].iterrows():
                    s = seqs[row.query_id][row.query_start:row.query_end]
                    s.id += f':{row.query_start}-{row.query_end}'
                    s.description = ''
                    SeqIO.write(s, out, format='fasta')


checkpoint hmmfetch:
    input:
        hmm_db = HMM_DB,
        besthits = f'{OUT_DIR}/proteins.domains.besthit.nooverlap.tsv'
    output:
        directory(f'{dirname(HMM_DB)}/profiles'),
    conda:
        'workflow-env.yaml'
    shell:
        'PROFILE_LIST="{input.besthits}.profiles";'
        'OUT_DIR=$(dirname "{output.profiles}");'
        'tail -n+2 {input.besthits} | cut -f4 | sort | uniq > $PROFILE_LIST;'
        'while read LINE; do hmmfetch {input.hmm_db} $LINE > $OUT_DIR/$LINE.hmm; done < $PROFILE_LIST;'
        'rm -f $PROFILE_LIST'


rule hmmalign:
    input:
        hmm_profile = f'{dirname(HMM_DB)}/profiles/{{profile}}.hmm',
        domains = f'{OUT_DIR}/domains/{{family}}.{{profile}}.faa'
    output:
        f'{OUT_DIR}/domains/{{family}}.{{profile}}.aln'
    conda:
        'workflow-env.yaml'
    wildcard_constraints:
        profile = r'PF\d+\.\d+'
    shell:
        'hmmalign {input.hmm_profile} {input.domains} > {output}'


rule concat_domain_alns:
    input:
        get_aln_files_for_family
    output:
        f'{OUT_DIR}/{{family}}.phy'
    run:
        from Bio import AlignIO, SeqIO
        from Bio.SeqRecord import SeqRecord
        from Bio.Seq import Seq
        from io import StringIO
        alns = dict()
        c = 0
        for f in input:
            for a in AlignIO.parse(open(f), format='stockholm'):
                cur_c = 0
                for s in SeqIO.parse(StringIO(format(a, 'fasta')), format='fasta'):
                    s.id = s.id.split(':', 1)[0]
                    s.description == ''
                    if s.id in alns:
                        alns[s.id] += s
                    else:
                        alns[s.id] = SeqRecord(Seq('-'*c + str(s.seq)), id=s.id, description='')
                    cur_c = max(cur_c, len(alns[s.id]) - c)
                c += cur_c
                for sid, s in alns.items():
                    if len(s) < c:
                        alns[sid] += Seq('-'*(c-len(s)))

        mfasta = StringIO()
        for sid in sorted(alns.keys()):
            SeqIO.write(alns[sid], mfasta, format='fasta')
        mfasta.seek(0)
        with open(output[0], 'w') as out:
            AlignIO.write(AlignIO.parse(mfasta, format='fasta'), out, 'phylip-relaxed')

